mat_q3 <- as.matrix(dfm_q3)
doc1 = mat_q3[1, ]
doc2 = mat_q3[2, ]
# for loop to calculate euclidean dist
euc_sum = 0
for(i in 1:dim(mat_q3)[2]){
euc_sum = euc_sum + (doc1[i] - doc2[i])^2
}
euc_dist <- sqrt(euc_sum)
cos_num = 0
doc1_norm_sq = 0
doc2_norm_sq = 0
for(i in 1:dim(mat_q3)[2]){
man_dist = man_dist + abs(doc1[i] - doc2[i])
cos_num = cos_num + doc1[i] * doc2[i]
doc1_norm_sq = doc1_norm_sq + doc1[i]^2
doc2_norm_sq = doc2_norm_sq + doc2[i]^2
}
cos_num / (sqrt(doc1_norm_sq) * sqrt(doc2_norm_sq))
library(gutenbergr)
n<-gutenberg_authors[,]
# list of authors
author_list <- c("Poe, Edgar Allan", "Twain, Mark", "Shelley, Mary Wollstonecraft","Doyle, Arthur Conan")
#Here a list of the gutenberg_id associated with the books is given below
book_list<-c(932,1064,1065,32037,74,76,86,91,84,6447,15238,18247,108,126,139,244)
#The gutenberg_id above were obtained with the following command
meta <- gutenberg_works(author == "Doyle, Arthur Conan") %>% slice(1:4)
meta
#The gutenberg_id above were obtained with the following command
meta <- gutenberg_works(author == "Doyle, Arthur Conan") %>% slice(1:4)
meta
# Prepare data function
# @param author_name: author’s name as it would appear in gutenberg
# @param num_texts: numeric specifying number of texts to select
# @param num_lines: num_lines specifying number of sentences to sample
meta <- gutenberg_works(gutenberg_id == book_list)
# Prepare data function
# @param author_name: author’s name as it would appear in gutenberg
# @param num_texts: numeric specifying number of texts to select
# @param num_lines: num_lines specifying number of sentences to sample
meta <- gutenberg_works(gutenberg_id == book_list)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
library(stringr)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
prepare_dt <- function(book_list, num_lines, removePunct = TRUE){
meta <- gutenberg_works(gutenberg_id == book_list)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
texts <- lapply(book_list, function(x) gutenberg_download(x,    mirror="http://mirrors.xmission.com/gutenberg/") %>%
#select(text) %>%
sample_n(500, replace=TRUE) %>%
unlist() %>%
paste(., collapse = " ") %>%
str_replace_all(., "^ +| +$|( ) +", "\\1"))
# remove apostrophes
texts <- lapply(texts, function(x) gsub("‘|’", "", x))
if(removePunct) texts <- lapply(texts, function(x)
gsub("[^[:alpha:]]", " ", x))
# remove all non-alpha characters
output <- tibble(title = meta$title, author = meta$author, text =
unlist(texts, recursive = FALSE))
}
# run function
set.seed(1984L)
texts_dt <- lapply(book_list, prepare_dt, num_lines = 500, removePunct = TRUE)
texts_dt <- do.call(rbind, texts_dt)
print(texts_dt$title)
print(texts_dt$author)
texts_dt
class(texts_dt)
texts_dt[1]
class(texts_dt[1])
data.frame(texts_dt)
df_q4 <- data.frame(texts_dt)
class(df_q4)
df_q4
View(df_q4)
str(df_q4)
df_q4
vocab_custom <- stylest_select_vocab(df_q4$text, df_q4$author,  # fits n-fold cross-validation
filter = filter, smooth = 1, nfold = 5,
cutoff_pcts = c(25, 50, 75, 99))
# Tokenization selections can optionally be passed as the filter argument
filter <- corpus::text_filter(drop_punct = TRUE, drop_number = TRUE, drop = stopwords_en)
stopwords_en <- stopwords("en")
# Tokenization selections can optionally be passed as the filter argument
filter <- corpus::text_filter(drop_punct = TRUE, drop_number = TRUE, drop = stopwords_en)
vocab_custom <- stylest_select_vocab(df_q4$text, df_q4$author,  # fits n-fold cross-validation
filter = filter, smooth = 1, nfold = 5,
cutoff_pcts = c(25, 50, 75, 99))
vocab_custom
?text_filter
?stylest_select_vocab
print(vocab_custom$miss_pct)
# subset features
vocab_subset <- stylest_terms(df_q4$text, df_q4$author, vocab_custom$cutoff_pct_best , filter = filter)
# fit model with "optimal" percentile threshold (i.e. feature subset)
style_model <- stylest_fit(df_q4$text, df_q4$author, terms = vocab_subset, filter = filter)
summary(style_model)
style_model
# explore output
head(stylest_term_influence(style_model, df_q4$text, df_q4$author))  # influential terms
style_model$rate
style_model
summary(style_model)
class(style_model)
style_model$rate
view(style_model$rate)
view(style_model$rate)
View(style_model)
style_model[[rate]]
style_model[['rate']]
style_model[["rate"]]
?stylest_fit
style_model[['rate']][1,1]
style_model[['rate']][:,:]
style_model[['rate']][1, 2]
style_model[['rate']][2, 1]
style_model[['rate']][1, :5]
style_model[['rate']][1, 1:5]
style_model[['rate']][1:5, 1]
style_model[['rate']][2, 1]
style_model[['rate']][3, 1]
style_model[['rate']][5, 1]
style_model[['rate']][4, 1]
data.frame(style_model[['rate']])
# report top 5 terms
data.frame(style_model[['rate']])[1:4, 1:5]
# report top 5 terms
data.frame(style_model[['rate']])
# report top 5 terms
data.frame(style_model[['rate']])[1,]
# report top 5 terms
data.frame(style_model[['rate']])[2,]
# report top 5 terms
data.frame(style_model[['rate']])
# report top 5 terms
rate_mat <- data.frame(style_model[['rate']])
View(rate_mat)
authors <- unique(df_q4$author)
term_usage <- style_model$rate
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) %>% setNames(authors)
term_usage
rate_mat
rate_mat[1, ]
rate_mat[2, ]
ate_mat[3, ]
rate_mat[3, ]
rate_mat[4, ]
vec_poe <- rate_mat[2, ]
vec_twain <- rate_mat[4, ]
class(vec_twain)
vec_poe/vec_twain
vec_poe_to_twian <- vec_poe/vec_twain
vec_poe_to_twian
authors
# arrange and extract top 5
new_authors <- c("poe", "twain")
lapply(new_authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) %>% setNames(new_authors)
lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)
vec_poe_to_twian
lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)
lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)[2]
lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)
lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)[[2]]
a <- lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)
class(a)
a[[2]]
a$twain
a$poe
d
sorted_ratios <- lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)
sorted_ratios <- lapply(new_authors, function(x) head(vec_poe_to_twian[x,][order(-vec_poe_to_twian[x,])])) %>% setNames(new_authors)
sorted_ratios$poe[1:5]
# load mystery
mystery <- readRDS('mystery_excerpt.rds')
mystery
?readRDS
pred <- stylest_predict(style_model, mystery)
pred$predicted
pred$log_probs
texts_dt
textstat_collocations(texts_dt)
dtm(texts_dt)
dfm(texts_dt)
texts_dt
class(dtm(texts_dt))
class(texts_dt)
data.frame(text_dt)
data.frame(texts_dt)
dfm(df_q4)
textstat_collocations
df_14
df_q4
View(df_q4)
trump_2018_text
# 1.1 load the State of the Union (SOTU) corpus ---------------------
sotu <- corpus(data_corpus_sotu)
# keep only the text of the the 2018 SOTU
trump_2018_text <- texts(trump_sotu)[2]
trump_sotu <- corpus_subset(sotu, President == "Trump")
# keep only the text of the the 2018 SOTU
trump_2018_text <- texts(trump_sotu)[2]
trump_2018_text
class(trump_2018_text)
dfm_q4 <- dfm(df_q4$text)
dfm_q4
# bigrams
head(textstat_collocations(dfm_q4))
# bigrams
head(textstat_collocations(df_q4$text))
?textstat_collocations
# bigrams
head(textstat_collocations(df_q4$text), min_count = 5)
# bigrams
bigram <- head(textstat_collocations(df_q4$text), min_count = 5)
class(bigram)
bigram
bigram[order(-bigram[lambda,])]
bigram
bigram[order(-bigram["lambda",])]
bigram[order(bigram["lambda"])]
order(bigram$lambda)
bigram
bigram[order(bigram$lambda)]
# bigrams
bigram <- textstat_collocations(df_q4$text, min_count = 5)
bigram[order(bigram$lambda)]
?order
bigram[-order(bigram$lambda), ]
bigram[order(-bigram$lambda), ]
# top 10 lambda
bigram[order(-bigram$lambda), ]$collocation
# top 10 lambda
print(head(bigram[order(-bigram$lambda), ]$collocation))
# top 10 lambda
print(head(bigram[order(-bigram$lambda), ]$collocation), 10)
# top 10 lambda
print(head(bigram[order(-bigram$lambda), ]$collocation, 10))
bigram
print(head(
bigram[order(-bigram$lambda), ]
# top 10 count
print(head(bigram[order(-bigram$count), ]$collocation, 10))
devtools::install_github("kbenoit/sophistication")
library("sophistication")
# Load data
data(data_corpus_ungd2017, package = "quanteda.corpora")
# Load data
q5_data <- data(data_corpus_ungd2017, package = "quanteda.corpora")
q5_data
# Load data
q5_data <- corpus(data_corpus_ungd2017)
q5_data
# Make snippets of 1 sentence each, then clean them
snippetData <- snippets_make(q5_data, minchar = 150, maxchar = 350)
snippetData <- snippets_clean(snippetData)
head(snippetData, 10)
# Sample the snippets
testData <- sample_n(snippetData, 1000)
snippetPairsMST
# generate n-1 pairs from n test snippets for a minimum spanning tree
snippetPairsMST <- pairs_regular_make(testData)
snippetPairsMST
gold_questions <- pairs_gold_make(snippetPairsAll, n.pairs = 10)
gold_questions <- pairs_gold_make(snippetPairsMST, n.pairs = 10)
View(gold_questions)
print(gold_questions[, c("text1", "text2")])
print(gold_questions[, c("text1", "text2")])
gold_questions[, c("text1", "text2")]
print(gold_questions[, c("text1", "text2")])
gold_questions[, c("text1", "text2")][1]
gold_questions[, c("text1", "text2")][,1]
gold_questions[, c("text1", "text2")]
library(pbapply)
new_book_list <- c(514, 64317)
new_meta <- gutenberg_works(gutenberg_id == new_book_list)
new_meta <- new_meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
book_list <- c(514, 64317)
meta <- gutenberg_works(gutenberg_id == new_book_list)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
# run function
set.seed(1984L)
q6_dt <- lapply(book_list, prepare_dt, num_lines = 500, removePunct = TRUE)
q6_dt <- do.call(rbind, q6_dt)
print(q6_dt$title)
print(q6_dt$author)
meta <- gutenberg_works(gutenberg_id == new_book_list)
book_list <- c(514, 64317)
meta <- gutenberg_works(gutenberg_id == new_book_list)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
# run function
set.seed(1984L)
q6_dt <- lapply(book_list, prepare_dt, num_lines = 500, removePunct = TRUE)
q6_dt <- do.call(rbind, q6_dt)
print(q6_dt$title)
print(q6_dt$author)
# Prepare data function
prepare_dt <- function(book_id, removePunct = TRUE){
#meta <- gutenberg_works(gutenberg_id  == book_id)
#meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
text <- gutenberg_download(book_id, mirror="http://mirrors.xmission.com/gutenberg/") %>%
select(text) %>%
filter(text!="") %>%
unlist() %>%
paste(., collapse = " ") %>%
str_replace_all(., "^ +| +$|( ) +", "\\1")
text <- gsub("`|'", "", text) # remove apostrophes
text <- gsub("[^[:alpha:]]", " ", text) # remove all non-alpha characters
output <- tibble(text = text)
}
#title = meta$title, author = meta$author,
title <- c("Little Women", "The Great Gatsby")
# run function
pair_texts <- lapply(c(514, 64317), prepare_dt, removePunct = TRUE)
pair_texts <- do.call(rbind, pair_texts)
pair_texts$title <- title
pair_texts$author <- author
author <- c("alcott", "fitzgerald")
#title = meta$title, author = meta$author,
title <- c("Little Women", "The Great Gatsby")
pair_texts$title <- title
pair_texts$author <- author
print(pair_texts_dfm)
# Prepare data function
prepare_dt <- function(book_id, removePunct = TRUE){
#meta <- gutenberg_works(gutenberg_id  == book_id)
#meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
text <- gutenberg_download(book_id, mirror="http://mirrors.xmission.com/gutenberg/") %>%
select(text) %>%
filter(text!="") %>%
unlist() %>%
paste(., collapse = " ") %>%
str_replace_all(., "^ +| +$|( ) +", "\\1")
text <- gsub("`|'", "", text) # remove apostrophes
text <- gsub("[^[:alpha:]]", " ", text) # remove all non-alpha characters
output <- tibble(text = text)
}
#title = meta$title, author = meta$author,
title <- c("Little Women", "The Great Gatsby")
author <- c("alcott", "fitzgerald")
# run function
pair_texts <- lapply(c(514, 64317), prepare_dt, removePunct = TRUE)
pair_texts <- do.call(rbind, pair_texts)
pair_texts$title <- title
pair_texts$author <- author
print(pair_texts_dfm)
# create dfm
pair_texts_dfm <- dfm(pair_texts$text, remove_punct = TRUE, tolower = TRUE)
print(pair_texts_dfm)
pair_texts
plot(log10(1:100), log10(topfeatures(pair_texts_dfm, 100)),
xlab = "log10(rank)", ylab = "log10(frequency)", main = " ")
plot(log10(1:100), log10(topfeatures(pair_texts_dfm, 100)),
xlab = "log10(rank)", ylab = "log10(frequency)", main = " ")
# Fits a linear regression to check if slope is approx -1.0
regression_q6 <- lm(log10(topfeatures(pair_texts_dfm, 100)) ~ log10(1:100))
# Adds the fitted line from regression to the plot
abline(regression_q6, col = "red")
# M = kT^b, k=44
summary(regression_q6)
tokens(pair_texts_dfm, remove_punct = TRUE)
pair_texts_dfm
?tokens
pair_texts
tokens_q7 <- tokens(pair_texts, remove_punct = TRUE)
tokens_q7 <- tokens(pair_texts$text, remove_punct = TRUE)
tokens_q7
num_tokens_q7 <- sum(lengths(tokens_q7))
num_tokens_q7
# t^b = M/k -> b = logt(M/k)
logb(M/k, base = tokens_q7)
M <- nfeat(pair_texts_dfm)
k <- 44
tokens_q7 <- tokens(pair_texts$text, remove_punct = TRUE)
num_tokens_q7 <- sum(lengths(tokens_q7))
# t^b = M/k -> b = logt(M/k)
logb(M/k, base = tokens_q7)
M/k
# t^b = M/k -> b = logt(M/k)
logb((M/k), base = num_tokens_q7)
tokens_q7 <- tokens(pair_texts$text, remove_punct = TRUE, tolower = TRUE)
# t^b = M/k -> b = logt(M/k)
logb((M/k), base = num_tokens_q7)
plot(1:100, topfeatures(pair_texts_dfm, 100),
xlab = "rank", ylab = "frequency", main = "")
topfeatures(pair_texts_dfm, 100)
pair_texts_dfm
# load data
data("data_corpus_ukmanifestos")
manifestos <- corpus_subset(data_corpus_ukmanifestos, Party == "Con")
# tokenize by sentences
sent_tokens <- unlist(tokens(manifestos, what = "sentence",
include_docvars = TRUE))
# extract year metadata
yearnames <- list(unlist(names(sent_tokens)))
yearnames <- lapply(yearnames[[1]], function(x){strsplit(x, "_")[[1]][3]})
yearslist <- unlist(yearnames)
# create tibble
sentences_df <- tibble(text = sent_tokens, year = yearslist)
# filter out non-sentences (only sentences that end in sentence punctuation
sentences_df <- sentences_df[grepl( ("[\\.\\?\\!]$"), sentences_df$text), ]
# create quanteda corpus object
#sent_corp <- corpus(sentences_df$text)
#docvars(sent_corp, field = "Year") <- sentences_df$year
# create quanteda corpus object
sent_corp <- corpus(sentences_df$text)
docvars(sent_corp, field = "Year") <- sentences_df$year
sent_corp
# Let's filter out any NAs
sentences_df <- na.omit(sentences_df)
# mean Flesch statistic per year
flesch_point <- sentences_df$texts %>% textstat_readability(measure = "Flesch") %>%
group_by(sentences_df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>% arrange(year)
flesch_point
# mean Flesch statistic per year
flesch_point <- sentences_df$text %>% textstat_readability(measure = "Flesch") %>%
group_by(sentences_df$year) %>%
summarise(mean_flesch = mean(Flesch)) %>%
setNames(c("year", "mean")) %>% arrange(year)
flesch_point
# We will use a loop to bootstrap a sample of texts and subsequently calculate standard errors
iters <- 10
yearnames
yearslist
# We will use a loop to bootstrap a sample of texts and subsequently calculate standard errors
iters <- 10
# build function to be used in bootstrapping
boot_flesch <- function(party_data){
N <- nrow(party_data)
bootstrap_sample <- corpus_sample(corpus(c(party_data$text)), size = N, replace = TRUE)
bootstrap_sample<- as.data.frame(as.matrix(bootstrap_sample))
readability_results <- textstat_readability(bootstrap_sample$V1, measure = "Flesch")
return(mean(readability_results$Flesch))
}
# apply function to each year
boot_flesch_by_year <- pblapply(yearslist, function(x){
sub_data <- sentences_df %>% filter(year == x)
output_flesch <- lapply(1:iters, function(i) boot_flesch(sub_data))
return(unlist(output_flesch))
})
nrow(party_data)
nrow(sentences_df)
yearslist
names(boot_flesch_by_year) <- unique(yearslist)
# apply function to each year
boot_flesch_by_year <- pblapply(yearslist, function(x){
sub_data <- sentences_df %>% filter(year == x)
output_flesch <- lapply(1:iters, function(i) boot_flesch(sub_data))
return(unlist(output_flesch))
})
unique(yearslist)
# apply function to each year
boot_flesch_by_year <- pblapply(unique(yearslist), function(x){
sub_data <- sentences_df %>% filter(year == x)
output_flesch <- lapply(1:iters, function(i) boot_flesch(sub_data))
return(unlist(output_flesch))
})
names(boot_flesch_by_year) <- unique(yearslist)
View(boot_flesch_by_year)
year_means <- lapply(boot_flesch_by_year, mean) %>% unname() %>% unlist()
year_ses   <- lapply(boot_flesch_by_year, sd) %>% unname() %>% unlist() # bootstrap standard error = sample standard deviation bootstrap distribution
year_means
year_ses
# Plot results--party
plot_dt <- tibble(year = unique(yearslist), mean = year_means, ses = year_ses)
plot_dt
boot_flesch_by_year
flesch_point
colnames(plot_dt) <- c('year', 'bootstrap_mean')
plot_dt
merge(flesch_point, plot_dt)
colnames(plot_dt) <- c('year', 'bootstrap_mean', 'bootstrap_se')
merge(flesch_point, plot_dt)
merge(flesch_point, plot_dt)[, 1:3]
pair_texts
kwic(pair_texts$text)
kwic(pair_texts$text, valuetype = 'regex')
kwic(pair_texts$text, pattern = 'class', valuetype = 'regex')
weighted_speeches_dfm
dfm_q3
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) %>% setNames(authors)
term_usage
# report top 5 terms
authors <- unique(df_q4$author)
head(snippetData, 10)
# Load data
q5_data <- corpus(data_corpus_ungd2017)
# Make snippets of 1 sentence each, then clean them
snippetData <- snippets_make(q5_data, minchar = 150, maxchar = 350)
snippetData <- snippets_clean(snippetData)
head(snippetData, 10)
regression_q6
# subset features
vocab_subset <- stylest_terms(df_q4$text, df_q4$author, vocab_custom$cutoff_pct_best , filter = filter)
# fit model with "optimal" percentile threshold (i.e. feature subset)
style_model <- stylest_fit(df_q4$text, df_q4$author, terms = vocab_subset, filter = filter)
stylest_term_influence(style_model, )
# report top 5 terms
authors <- unique(df_q4$author)
term_usage <- style_model$rate
lapply(authors, function(x) head(term_usage[x,][order(-term_usage[x,])])) %>% setNames(authors)
kwic(pair_texts$text, pattern = 'money', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'status', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'status', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'state', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'high', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'wealth', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'poor', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'class', valuetype = 'regex')
kwic(pair_texts$text, pattern = 'poor', valuetype = 'regex')
